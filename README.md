# Vision Transformer + BART for Image Captioning Generation

## Overview
This repository showcases the implementation of an image captioning model that consists of a Vision Transformer (ViT) as the encoder and a BART transformer as the decoder. This model is trained on the MS COCO 2017 dataset to generate captions for the respective images.

This project aims at generating accurate and meaningful image captions, that can be applied in tasks such as visual storytelling, accessibility for visually impaired individuals, and multimedia indexing.


## Features
* Utilizes the Vision Transformer(ViT) for feature extraction of the images.
* BART, a pre-trained sequence-to-sequence model, for caption generation.
* Trained and fine-tuned on the MS COCO 2017 dataset.
* Some of the evaluation metrics included are BLEU, ROUGE, and METEOR.


## Prerequisites
* Python 3.8 or higher.
* PyTorch 2.0 or higher.
* CUDA.

## Architecture
![IMG-20241221-WA0016](https://github.com/user-attachments/assets/45da9f80-4e16-44b6-8ab4-a570310ea472)



  
